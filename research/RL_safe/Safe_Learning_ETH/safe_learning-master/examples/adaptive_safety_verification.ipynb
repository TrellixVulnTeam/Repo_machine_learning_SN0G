{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Safety Verification for the Inverted Pendulum\n",
    "\n",
    "Determine the largest safe set for a GP model of the inverted pendulum with an adaptive discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import safe_learning\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from utilities import InvertedPendulum, binary_cmap\n",
    "\n",
    "# Nice progress bars\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "\n",
    "_STORAGE = {}\n",
    "\n",
    "HEAT_MAP = plt.get_cmap('inferno', lut=None)\n",
    "HEAT_MAP.set_over('white')\n",
    "HEAT_MAP.set_under('black')\n",
    "\n",
    "LEVEL_MAP = plt.get_cmap('viridis', lut=21)\n",
    "LEVEL_MAP.set_over('gold')\n",
    "LEVEL_MAP.set_under('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = safe_learning.config.np_dtype,\n",
    "                  tf_dtype              = safe_learning.config.dtype,\n",
    "                  saturate              = True,                            # apply saturation constraints to the control input\n",
    "                  eps                   = 1e-8,                            # numerical tolerance\n",
    "                  use_linear_dynamics   = False,                           # use the linearized form of the dynamics as the true dynamics (for testing)\n",
    "                  use_lipschitz_scaling = True,                            # use different Lipschitz constants in each state for the Lyapunov function\n",
    "                  use_zero_threshold    = False,                           # assume the discretization is infinitely fine (i.e., tau = 0; for testing)\n",
    "                  use_true_parameters   = False,                           # use the true physical parameters in the GP model (for testing)\n",
    "                  use_linear_kernels    = False,                           # use only linear kernels in the GP model\n",
    "                  use_adaptive_grid     = True,                            # use an adaptive discretization for safety verification\n",
    "                  gp_confidence_scaling = 2.,                              # scaling factor for GP confidence intervals (i.e., beta)\n",
    "                  gp_noise_variance     = 0.001 ** 2,                      # noise variance used in GP model\n",
    "                  gp_num_scaling        = 1.,                              # internal scaling factor for better numerical stability in GP prediction\n",
    "                  dpi                   = 200,\n",
    "                  num_cores             = 4,\n",
    "                  num_sockets           = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session\n",
    "\n",
    "Customize the TensorFlow session for the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_BLOCKTIME\"]    = str(0)\n",
    "os.environ[\"KMP_SETTINGS\"]     = str(1)\n",
    "os.environ[\"KMP_AFFINITY\"]     = 'granularity=fine,noverbose,compact,1,0'\n",
    "os.environ[\"OMP_NUM_THREADS\"]  = str(OPTIONS.num_cores)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads  = OPTIONS.num_cores,\n",
    "                        inter_op_parallelism_threads  = OPTIONS.num_sockets,\n",
    "                        allow_soft_placement          = False,\n",
    "                        device_count                  = {'CPU': OPTIONS.num_cores})\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "session = tf.InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "\n",
    "Define the nonlinear and linearized forms of the inverted pendulum dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.15    # pendulum mass\n",
    "L = 0.5     # pole length\n",
    "b = 0.1     # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(30)                 # angular position [rad]\n",
    "omega_max = np.sqrt(g / L)                 # angular velocity [rad/s]\n",
    "u_max     = g * m * L * np.sin(theta_max)  # torque [N.m], control action\n",
    "\n",
    "state_norm  = (theta_max, omega_max)\n",
    "action_norm = (u_max,)\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim     = 2\n",
    "action_dim    = 1\n",
    "state_limits  = np.array([[-1., 1.]] * state_dim)\n",
    "action_limits = np.array([[-1., 1.]] * action_dim)\n",
    "\n",
    "# True system\n",
    "true_pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A_true, B_true = true_pendulum.linearize()\n",
    "\n",
    "if OPTIONS.use_linear_dynamics:\n",
    "    true_dynamics = safe_learning.functions.LinearSystem((A_true, B_true), name='true_dynamics')\n",
    "else:\n",
    "    true_dynamics = true_pendulum.__call__\n",
    "\n",
    "if not OPTIONS.use_true_parameters:\n",
    "    # \"Wrong\" system\n",
    "    m = 0.1     # pendulum mass\n",
    "    L = 0.4     # pole length\n",
    "    b = 0.0     # rotational friction\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A, B = pendulum.linearize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Model\n",
    "\n",
    "Define a GP model with possibly wrong physical parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior variances; make sure at least some non-zero value is maintained\n",
    "M_true = np.hstack((A_true, B_true))\n",
    "M = np.hstack((A, B))\n",
    "prior_variances = (M_true - M) ** 2\n",
    "np.clip(prior_variances, 1e-3, None, out=prior_variances)\n",
    "\n",
    "# Input to GP is of the form (x, u) = (state, action)\n",
    "full_dim = state_dim + action_dim\n",
    "\n",
    "# Kernels\n",
    "if OPTIONS.use_linear_kernels:\n",
    "    kernel_theta = gpflow.kernels.Linear(full_dim, variance=prior_variances[0, :], ARD=True)\n",
    "    kernel_omega = gpflow.kernels.Linear(full_dim, variance=prior_variances[1, :], ARD=True)\n",
    "else:\n",
    "    kernel_theta = (gpflow.kernels.Linear(full_dim, variance=prior_variances[0, :], ARD=True)\n",
    "                    + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "                    * gpflow.kernels.Linear(1, variance=prior_variances[0, 1]))\n",
    "    kernel_omega = (gpflow.kernels.Linear(full_dim, variance=prior_variances[1, :], ARD=True)\n",
    "                    + gpflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "                    * gpflow.kernels.Linear(1, variance=prior_variances[1, 1]))\n",
    "\n",
    "# Use linearized form for the mean dynamics\n",
    "mean_function_theta = safe_learning.LinearSystem((A[[0], :], B[[0], :]), name='mean_dynamics_theta')\n",
    "mean_function_omega = safe_learning.LinearSystem((A[[1], :], B[[1], :]), name='mean_dynamics_omega')\n",
    "\n",
    "# TODO Tensorflow may spit out a lot of allocator errors when creating 0-length dataholders in gpflow, e.g., when:\n",
    "#     - initializing with empty data matrices X and Y, or\n",
    "#     - using GPRCached (initializes empty dataholders for Cholesky decomposition)\n",
    "\n",
    "# X_init = np.empty((0, full_dim), dtype=OPTIONS.np_dtype)\n",
    "# Y_init = np.empty((0, 1), dtype=OPTIONS.np_dtype)\n",
    "# gp_theta = safe_learning.GPRCached(X_init, Y_init, kernel_theta, mean_function_theta, OPTIONS.gp_num_scaling)\n",
    "# gp_omega = safe_learning.GPRCached(X_init, Y_init, kernel_omega, mean_function_omega, OPTIONS.gp_num_scaling)\n",
    "\n",
    "# Define a GP model over the dynamics\n",
    "X_init   = np.zeros((1, full_dim), dtype=OPTIONS.np_dtype)\n",
    "Y_init   = np.zeros((1, 1), dtype=OPTIONS.np_dtype)\n",
    "\n",
    "gp_theta = gpflow.gpr.GPR(X_init, Y_init, kernel_theta, mean_function_theta)\n",
    "gp_omega = gpflow.gpr.GPR(X_init, Y_init, kernel_omega, mean_function_omega)\n",
    "\n",
    "gp_theta.likelihood.variance = OPTIONS.gp_noise_variance\n",
    "gp_omega.likelihood.variance = OPTIONS.gp_noise_variance\n",
    "\n",
    "gp_theta_fun = safe_learning.GaussianProcess(gp_theta, OPTIONS.gp_confidence_scaling)\n",
    "gp_omega_fun = safe_learning.GaussianProcess(gp_omega, OPTIONS.gp_confidence_scaling)\n",
    "\n",
    "# Stack GP functions to get a block-diagonal kernel matrix, which yields more efficient GP prediction\n",
    "dynamics = safe_learning.FunctionStack((gp_theta_fun, gp_omega_fun))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization and Initial Safe Set\n",
    "\n",
    "Define a possibly adaptive discretization, and an initial known safe set as a subset of this discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states along each dimension\n",
    "if OPTIONS.use_adaptive_grid:\n",
    "    num_states = 501\n",
    "else:\n",
    "    num_states = 3001\n",
    "\n",
    "# State grid\n",
    "grid_limits = np.array([[-1., 1.], ] * state_dim)\n",
    "grid = safe_learning.GridWorld(grid_limits, num_states)\n",
    "\n",
    "# Discretization constant\n",
    "if OPTIONS.use_zero_threshold:\n",
    "    tau = 0.0\n",
    "else:\n",
    "    tau = np.sum(grid.unit_maxes) / 2\n",
    "\n",
    "print('Grid size: {}'.format(grid.nindex))\n",
    "print('Discretization constant (tau): {}'.format(tau))\n",
    "\n",
    "# Set initial safe set as a ball around the origin (in normalized coordinates)\n",
    "cutoff_radius    = 0.2\n",
    "initial_safe_set = np.linalg.norm(grid.all_points, ord=2, axis=1) <= cutoff_radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Policy\n",
    "\n",
    "Fix the policy to the LQR solution for the linearized, discretized, true system, possibly with saturation constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.diag([1., 2.]).astype(OPTIONS.np_dtype)                # state cost matrix\n",
    "R = 1.2 * np.identity(action_dim).astype(OPTIONS.np_dtype)    # action cost matrix\n",
    "K, P = safe_learning.utilities.dlqr(A_true, B_true, Q, R)\n",
    "P /= np.abs(P).max()                                          # normalize cost\n",
    "\n",
    "policy = safe_learning.LinearSystem(-K, name='policy')\n",
    "if OPTIONS.saturate:\n",
    "    policy = safe_learning.Saturation(policy, -1, 1)\n",
    "\n",
    "# Visualize policy\n",
    "def plot_policy(policy, grid, norms, tol=1e-10):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "    ticks = np.linspace(-1., 1., 9)\n",
    "    cutoff = 1. - tol\n",
    "    plot_limits = np.asarray(norms).reshape((-1, 1)) * grid.limits\n",
    "    \n",
    "    z = policy(grid.all_points).eval().reshape(grid.num_points)\n",
    "    im = ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=plot_limits[0, 1] / plot_limits[1, 1], cmap=HEAT_MAP, vmin=-cutoff, vmax=cutoff)\n",
    "    cbar = fig.colorbar(im, ax=ax, label=r'$u = \\pi(x)$ [normalized]', ticks=ticks)\n",
    "    ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "    ax.set_ylabel(r'$\\omega$ [deg/s]')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "norms = np.rad2deg(state_norm)\n",
    "plot_policy(policy, grid, norms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-Loop Dynamics Lipschitz Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy (linear)\n",
    "L_pol = np.linalg.norm(-K, 1)\n",
    "\n",
    "# Dynamics (linear approximation)\n",
    "L_dyn = np.linalg.norm(A_true, 1) + np.linalg.norm(B_true, 1) * L_pol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Lyapunov Function\n",
    "\n",
    "Fix the Lyapunov function to the LQR solution for the linearized, discretized, true system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lyapunov function corresponding to the LQR policy\n",
    "lyapunov_function = safe_learning.QuadraticFunction(P)\n",
    "\n",
    "# Approximate local Lipschitz constants with gradients\n",
    "grad_lyapunov_function = safe_learning.LinearSystem((2 * P,))\n",
    "if OPTIONS.use_lipschitz_scaling:\n",
    "    L_v = lambda x: tf.abs(grad_lyapunov_function(x))\n",
    "else:\n",
    "    L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keep_dims=True)\n",
    "\n",
    "# Initialize class (with a possibly adaptive discretization for safety verification)\n",
    "lyapunov = safe_learning.Lyapunov(grid, lyapunov_function, dynamics, L_dyn, L_v, tau, policy, initial_safe_set, adaptive=OPTIONS.use_adaptive_grid)\n",
    "lyapunov.update_values()\n",
    "lyapunov.update_safe_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current\n",
    "states = tf.placeholder(OPTIONS.tf_dtype, shape=[None, grid.ndim], name='states')\n",
    "actions = policy(states)\n",
    "values = lyapunov.lyapunov_function(states)\n",
    "\n",
    "# Predicted future\n",
    "future_states_mean, future_states_error = lyapunov.dynamics(states, actions)\n",
    "future_values_mean = lyapunov.lyapunov_function(future_states_mean)\n",
    "lv = lyapunov.lipschitz_lyapunov(future_states_mean)\n",
    "future_values_error = tf.reduce_sum(lv * future_states_error, axis=1, keepdims=True)\n",
    "dv_mean = future_values_mean - values\n",
    "dv_bound = dv_mean + future_values_error\n",
    "\n",
    "# True future\n",
    "future_states = true_dynamics(states, actions)\n",
    "future_values = lyapunov.lyapunov_function(future_states)\n",
    "dv = future_values - values\n",
    "\n",
    "# Discretization effects\n",
    "tau = tf.placeholder(OPTIONS.tf_dtype, shape=[None, 1], name='discretization_constant')\n",
    "threshold = lyapunov.threshold(states, tau)\n",
    "negative = tf.less(dv_bound, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Discretization Effects\n",
    "\n",
    "The tightened Lyapunov stability certificate $\\Delta v({\\bf x}) := v(f_\\pi({\\bf x})) - v({\\bf x}) < - L_{\\Delta v}\\tau$ becomes easier to satisfy as the grid is refined (i.e., as the spacing $\\tau$ decreases). However, this creates more states that must be verified within any level set due to the curse of dimensionality. For a given uniform grid with a side length of $M$ cells, $\\Delta v({\\bf x}) < - L_{\\Delta v}\\tau$ may not be satisfied, but \n",
    "    $$\\Delta v({\\bf x}) < - L_{\\Delta v}\\frac{\\tau}{N({\\bf x})}$$ \n",
    "may be, where $N({\\bf x}) \\in \\mathbb{N}_{\\geq 1}$ represents an adaptive refinement of the grid cell centred at $\\bf{x}$. This new condition would need to be checked at the $N({\\bf x})^d$ additional grid points created around $\\bf x$. We visualize the required refinement $N(\\bf{x})$ when beginning with a uniform square (i.e., $M^d$-sized) grid for the true dynamics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this parameter to see the effect of different uniform discretizations, and the required adaptive refinement. \n",
    "cells_per_side = 250\n",
    "\n",
    "# Initialize a uniform square grid\n",
    "grid_limits = np.array([[-1., 1.], ] * state_dim)\n",
    "grid = safe_learning.GridWorld(grid_limits, cells_per_side + 1)\n",
    "grid_spacing = np.sum(grid.unit_maxes) / 2\n",
    "\n",
    "# Create a colormap for N(x)\n",
    "N_max = 16\n",
    "cmap = plt.get_cmap('viridis', lut=N_max)\n",
    "cmap.set_over('gold')\n",
    "cmap.set_under((1., 1., 1., 0.))\n",
    "\n",
    "# Compute the required refinement N(x) for the adaptive discretization; if dv >= 0, then no amount of refinement will help, so we set N(x) = -1 (white) for plotting\n",
    "feed_dict = {states: grid.all_points, tau: [[np.sum(grid.unit_maxes) / 2]]}\n",
    "N = (threshold / dv).eval(feed_dict)\n",
    "N[np.isnan(N)] = -1\n",
    "N[N < 0] = -1\n",
    "N = np.ceil(N)\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=OPTIONS.dpi)\n",
    "\n",
    "z = N.reshape(grid.num_points)\n",
    "im = ax.imshow(z.T, origin='lower', extent=grid_limits.ravel(), aspect=grid_limits[0, 1] / grid_limits[1, 1], cmap=cmap, vmin=0, vmax=N_max)\n",
    "cbar = fig.colorbar(im, ax=ax, label=r'$N({\\bf x})$', ticks=np.arange(0, N_max + 1, 2))\n",
    "ax.set_title(r'$M = {}$'.format(grid.num_points[0] - 1) \n",
    "             + ', ' + r'$|\\mathcal{X}_\\tau|$ = ' + r'{:.1e}'.format(grid.nindex) \n",
    "             + ', ' + r'$\\tau$ = ' + r'{:.0e}'.format(grid_spacing), \n",
    "             )\n",
    "ax.set_xlabel(r'$\\theta$ [deg]')\n",
    "ax.set_ylabel(r'$\\omega$ [deg/s]')\n",
    "\n",
    "yticks = cbar.ax.get_yticks()\n",
    "tick_labels = ['{:.0f}'.format(y * N_max) for y in yticks]\n",
    "tick_labels[-1] = r'$\\geq {}$'.format(N_max)\n",
    "cbar.ax.set_yticklabels(tick_labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe Online Learning and Exploration\n",
    "\n",
    "Only visit certified safe states in order to obtain measurements and update the GP model of the dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not updating the policy, so do not consider perturbations around the current policy\n",
    "action_variation = np.array([[0.]], dtype=OPTIONS.np_dtype)\n",
    "\n",
    "with tf.name_scope('add_new_measurement'):\n",
    "    full_dim = state_dim + action_dim \n",
    "    tf_max_state_action = tf.placeholder(OPTIONS.tf_dtype, shape=[1, full_dim])\n",
    "    tf_measurement = true_dynamics(tf_max_state_action)\n",
    "    \n",
    "def update_gp():\n",
    "    \"\"\"Update the GP model based on an actively selected data point.\"\"\"\n",
    "    \n",
    "    # Get a new sample location\n",
    "    max_state_action, _ = safe_learning.get_safe_sample(lyapunov, action_variation, action_limits, positive=True, num_samples=1000)\n",
    "    \n",
    "    # Obtain a measurement of the true dynamics\n",
    "    lyapunov.feed_dict[tf_max_state_action] = max_state_action\n",
    "    measurement = tf_measurement.eval(feed_dict=lyapunov.feed_dict)\n",
    "    \n",
    "    # Add the measurement to our GP dynamics\n",
    "    lyapunov.dynamics.add_data_point(max_state_action, measurement)\n",
    "\n",
    "\n",
    "# Record some metrics during data collection\n",
    "safe_level = []         # current level c of the largest verifiable safe set V(c)\n",
    "safe_set_fraction = []  # current safe set size approximated as a fraction of the discretization that is considered safe\n",
    "num_measurements = []   # number of measurements collected\n",
    "update_count = 0        # number of safe set updates so far\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "This cell can be run repeatedly to collect more measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_update  = 10     # number of measurements to collect before attempting to update the safe set\n",
    "safe_set_updates = 12     # number of safe set updates\n",
    "can_shrink       = False  # whether or not to \"re-verify\" known safe states as the GP model is updated, \n",
    "                          # i.e., can the safe set shrink in volume? (use \"False\" for speed, \"True\" for testing)\n",
    "safety_factor    = 1.     # scaling factor used to conservatively estimate the required adaptive refinement\n",
    "N_max            = 16     # the maximum adaptive refinement N(x) to attempt; lower is faster, while higher allows larger safe sets to be verified \n",
    "\n",
    "for _ in range(safe_set_updates):\n",
    "    update_count += 1\n",
    "#     print('Iteration {} with current safe level: {}'.format(update_count, lyapunov.feed_dict[lyapunov.c_max]))\n",
    "\n",
    "    # Collect measurements for the GP model\n",
    "    start = time.time()\n",
    "    for _ in range(data_per_update): \n",
    "        update_gp()\n",
    "    end = time.time()\n",
    "    duration_gp = end - start\n",
    "    \n",
    "    # Update safe set\n",
    "    start = time.time()\n",
    "    lyapunov.update_safe_set(can_shrink, N_max, safety_factor, OPTIONS.num_cores)\n",
    "    end = time.time()\n",
    "    duration_lyap = end - start\n",
    "    \n",
    "    # Record metrics\n",
    "    safe_level.append(lyapunov.feed_dict[lyapunov.c_max])\n",
    "    safe_set_fraction.append(np.sum(lyapunov.safe_set) / lyapunov.discretization.nindex)\n",
    "    if update_count == 1:\n",
    "        num_measurements.append(data_per_update)\n",
    "    else:\n",
    "        num_measurements.append(num_measurements[-1] + data_per_update)\n",
    "    \n",
    "    print('Data points collected so far: {}'.format(num_measurements[-1]))\n",
    "    print('Safe set size (relative to grid): {:.2f}%'.format(np.sum(100 * safe_set_fraction[-1])))\n",
    "    print('Duration of GP update (avg): {}'.format(duration_gp / data_per_update))\n",
    "    print('Duration of safe set update: {}'.format(duration_lyap))\n",
    "    print(\"NEW safe level: {}\".format(lyapunov.feed_dict[lyapunov.c_max]))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Plot the largest verifiable safe set and the measurement points. If the discretization is adaptive, use a colormap to show how much refinement $N({\\bf x})$ was necessary to satisfy the tightened Lyapunov decrease condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = lyapunov.discretization\n",
    "feed_dict = lyapunov.feed_dict\n",
    "feed_dict[states] = grid.all_points\n",
    "feed_dict[tau] =  [[lyapunov.tau]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), dpi=OPTIONS.dpi)\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "plot_limits = np.rad2deg(state_norm).reshape((-1, 1)) * grid.limits\n",
    "axes[0].set_title(r'$M = {}$'.format(grid.num_points[0] - 1)\n",
    "             + ', ' + r'$|\\mathcal{X}_\\tau| =$ ' + r'{:.1e}'.format(grid.nindex)\n",
    "             + ', ' + r'$\\tau =$ ' + r'{:.0e}'.format(np.sum(grid.unit_maxes) / 2))\n",
    "axes[0].set_xlabel(r'$\\theta$ [deg]')\n",
    "axes[0].set_ylabel(r'$\\omega$ [deg/s]')\n",
    "\n",
    "axes[1].step(num_measurements, safe_set_fraction, 'o', where='post')\n",
    "axes[1].set_xlabel(r'number of measurements')\n",
    "axes[1].set_ylabel(r'safe set size [% of grid]')\n",
    "\n",
    "# Decrease region for the true dynamics\n",
    "decrease_region = (dv.eval(feed_dict) < 0).reshape(grid.num_points)\n",
    "cmap = binary_cmap('lightgrey')\n",
    "im = axes[0].imshow(decrease_region.T, origin='lower', extent=plot_limits.ravel(), aspect=plot_limits[0, 1] / plot_limits[1, 1], cmap=cmap, vmin=0, vmax=None)\n",
    "\n",
    "# Refinement N(x) used; colorbar shown only if the discretization is adaptive\n",
    "N = np.copy(lyapunov._refinement)\n",
    "N[N == 0] = -1 # for color only\n",
    "\n",
    "z = N.reshape(grid.num_points)\n",
    "cmap = plt.get_cmap('viridis', lut=N_max)\n",
    "cmap.set_over('gold')\n",
    "cmap.set_under((1., 1., 1., 0.))\n",
    "im = axes[0].imshow(z.T, origin='lower', extent=plot_limits.ravel(), aspect=plot_limits[0, 1] / plot_limits[1, 1], cmap=cmap, vmin=0, vmax=N_max)\n",
    "if OPTIONS.use_adaptive_grid:\n",
    "    cbar = fig.colorbar(im, ax=axes[0], label=r'$N({\\bf x})$', ticks=np.arange(0, N_max + 1, 2))\n",
    "\n",
    "# Initial safe set\n",
    "initial_safe_set = lyapunov.initial_safe_set.reshape(grid.num_points)\n",
    "cmap = binary_cmap('red')\n",
    "im = axes[0].imshow(initial_safe_set.T, origin='lower', extent=plot_limits.ravel(), aspect=plot_limits[0, 1] / plot_limits[1, 1], cmap=cmap, vmin=None, vmax=None)\n",
    "\n",
    "# Measurements\n",
    "if isinstance(lyapunov.dynamics, safe_learning.UncertainFunction):\n",
    "    # Skip origin data point\n",
    "    X = norms.ravel() * lyapunov.dynamics.functions[0].X[1:, :grid.ndim]\n",
    "    axes[0].plot(X[:, 0], X[:, 1], 'x', color='pink', mew=1, ms=6)\n",
    "\n",
    "# Legend\n",
    "colors = ['red', 'pink', 'lightgrey']\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in colors]\n",
    "labels = [r'Initial safe set', r'Measurements'.format(len(X)), r'$\\Delta v({\\bf x}) < 0$']\n",
    "axes[0].legend(proxy, labels, loc='lower left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
