{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition\n",
    "\n",
    "We define a reinforcement learning problem with piecewise linear function approximators. These rely on a regular discretization of the grid into cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = [[-1.2, 0.7], [-.07, .07]]\n",
    "n_points = [20, 20]\n",
    "\n",
    "# Define a discretization on the domain\n",
    "discretization = safe_learning.GridWorld(domain, n_points)\n",
    "\n",
    "# Value function is piecewise linear\n",
    "value_function = safe_learning.Triangulation(discretization, np.zeros(discretization.nindex), project=True,\n",
    "                                             name='tri_value_function')\n",
    "\n",
    "# Policy is piecewise linear and saturated\n",
    "policy = safe_learning.Triangulation(discretization, np.zeros(discretization.nindex), project=True,\n",
    "                                     name='tri_policy')\n",
    "policy = safe_learning.Saturation(policy, -1., 1.)\n",
    "\n",
    "# Discount factor\n",
    "gamma = .99\n",
    "terminal_reward = 1 - gamma\n",
    "\n",
    "@safe_learning.utilities.with_scope('true_dynamics')\n",
    "def dynamics(states, actions):\n",
    "    \"\"\"Return future states of the car\"\"\"    \n",
    "    x0 = states[:, 0] + states[:, 1]\n",
    "    x1 = states[:, 1] + 0.001 * actions[:, 0] - 0.0025 * tf.cos(3 * states[:, 0])\n",
    "    \n",
    "    return tf.stack((x0, x1), axis=1)\n",
    "\n",
    "\n",
    "@safe_learning.utilities.with_scope('reward_function')\n",
    "def reward_function(states, actions):\n",
    "    \"\"\"Reward function for the mountain car\"\"\"\n",
    "    zeros = tf.zeros((states.shape[0], 1), tf.float64)\n",
    "    ones = tf.ones_like(zeros)\n",
    "    # Reward is zero except at terminal states\n",
    "    return tf.where(tf.greater(states[:, 0], 0.6), terminal_reward * ones, zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the policy optimization problems\n",
    "\n",
    "Based on the dynamics we define the tensorflow operations to optimize the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reinforcement learning setup\n",
    "rl = safe_learning.PolicyIteration(\n",
    "    policy,\n",
    "    dynamics,\n",
    "    reward_function,\n",
    "    value_function,\n",
    "    gamma=gamma)\n",
    "\n",
    "# Create a tensorflow session\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Optimize over policy variables\n",
    "with tf.name_scope('dynamics_programming'):\n",
    "    # For triangulations we can solve a linear programm to determine the value function\n",
    "    # value_opt = rl.value_iteration()\n",
    "    value_opt = rl.optimize_value_function()\n",
    "    \n",
    "    # The policy is optimized using gradient descent\n",
    "    policy_loss = -1 / (1-gamma) * tf.reduce_mean(rl.future_values(rl.state_space))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.)\n",
    "    adapt_policy = optimizer.minimize(policy_loss,\n",
    "                                      var_list=[rl.policy.parameters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "old_values = np.zeros_like(rl.value_function.parameters[0].eval())\n",
    "old_actions = np.zeros_like(rl.policy.parameters[0].eval())\n",
    "converged = False\n",
    "action_space = np.array([[-1.], [1.]])\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    # Optimize value function\n",
    "    value_opt.eval()\n",
    "\n",
    "    # Optimize policy (discrete over grid or gradient descent)\n",
    "    # rl.discrete_policy_optimization(action_space)\n",
    "    for _ in range(200):\n",
    "        session.run(adapt_policy)\n",
    "\n",
    "    # Get new parameters\n",
    "    values, actions = session.run([rl.value_function.parameters[0],\n",
    "                                  rl.policy.parameters[0]])\n",
    "\n",
    "    # Compute errors\n",
    "    value_change = np.max(np.abs(old_values - values))\n",
    "    actions_change = np.max(np.abs(old_actions - actions))\n",
    "\n",
    "    # Break if converged\n",
    "    if value_change <= 1e-1 and actions_change <= 1e-1:\n",
    "        converged = True\n",
    "        break\n",
    "    else:\n",
    "        old_values = values\n",
    "        old_actions = actions\n",
    "\n",
    "\n",
    "if converged:\n",
    "    print('converged after {} iterations. \\nerror: {}, \\npolicy: {}'\n",
    "          .format(i + 1, value_change, actions_change))\n",
    "else:\n",
    "    print('didnt converge, error: {} and policy: {}'\n",
    "          .format(value_change, actions_change))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the resulting value function and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_triangulation(rl.value_function, xlabel='position', ylabel='velocity')\n",
    "plt.show()\n",
    "\n",
    "plotting.plot_triangulation(rl.value_function, three_dimensional=True,\n",
    "                            xlabel='position', ylabel='velocity', zlabel='values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_triangulation(rl.policy, zlabel='policy', xlabel='position', ylabel='velocity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('compute_trajectory'):\n",
    "    states = np.zeros((1000, 2), dtype=np.float)\n",
    "    states[0, 0] = -0.5\n",
    "\n",
    "    state = tf.placeholder(tf.float64, [1, 2])\n",
    "    next_states = rl.dynamics(state, rl.policy(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the trajectories.\n",
    "for i in range(len(states) - 1):\n",
    "    states[i+1, :] = next_states.eval(feed_dict={state: states[[i], :]})\n",
    "\n",
    "    # break if terminal\n",
    "    if states[i+1, 0] >= 0.6:\n",
    "        states[i+1:] = states[i+1]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plotting.plot_triangulation(rl.value_function, xlabel='position', ylabel='velocity')\n",
    "ax.plot(states[:,0], states[:, 1], lw=3, color='k')\n",
    "ax.plot(np.ones(2) * 0.6, ax.get_ylim(), lw=2, color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
