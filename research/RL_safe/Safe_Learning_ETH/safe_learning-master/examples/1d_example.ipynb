{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "np.random.seed(0)\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "Optimize over the policy such that the safe set does not shrink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a discretization of the space $[-1, 1]$ with discretization constant $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_min, x_max, discretization\\\n",
    "state_limits = np.array([[-1., 1.]])\n",
    "action_limits = np.array([[-.5, .5]])\n",
    "num_states = 1000\n",
    "num_actions = 101\n",
    "\n",
    "safety_disc = safe_learning.GridWorld(state_limits, num_states)\n",
    "\n",
    "# Discretization for optimizing the policy (discrete action space)\n",
    "# This is not necessary if one uses gradients to optimize the policy\n",
    "action_disc = safe_learning.GridWorld(action_limits, num_actions)\n",
    "\n",
    "# Discretization constant\n",
    "tau = np.max(safety_disc.unit_maxes)\n",
    "\n",
    "# Initial policy: All zeros\n",
    "policy_disc = safe_learning.GridWorld(state_limits, 51)\n",
    "policy = safe_learning.Triangulation(policy_disc, np.zeros(len(policy_disc)), name='policy')\n",
    "\n",
    "print('Grid size: {0}'.format(len(safety_disc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GP dynamics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = (gpflow.kernels.Matern32(2, lengthscales=1, active_dims=[0, 1]) *\n",
    "          gpflow.kernels.Linear(2, variance=[0.2, 1], ARD=True))\n",
    "\n",
    "noise_var = 0.01 ** 2\n",
    "\n",
    "# Mean dynamics\n",
    "mean_function = safe_learning.LinearSystem(([1, 0.1]), name='prior_dynamics')\n",
    "\n",
    "mean_lipschitz = 0.8\n",
    "gp_lipschitz = 0.5 # beta * np.sqrt(kernel.Mat32.variance) / kernel.Mat32.lengthscale * np.max(np.abs(state_limits))\n",
    "lipschitz_dynamics = mean_lipschitz + gp_lipschitz\n",
    "\n",
    "a = 1.2\n",
    "b = 1.\n",
    "q = 1.\n",
    "r = 1.\n",
    "\n",
    "true_dynamics = safe_learning.LinearSystem((a, b), name='true_dynamics')\n",
    "\n",
    "# Define a GP model over the dynamics\n",
    "gp = gpflow.gpr.GPR(np.empty((0, 2), dtype=safe_learning.config.np_dtype),\n",
    "                    np.empty((0, 1), dtype=safe_learning.config.np_dtype),\n",
    "                    kernel,\n",
    "                    mean_function=mean_function)\n",
    "gp.likelihood.variance = noise_var\n",
    "\n",
    "dynamics = safe_learning.GaussianProcess(gp, name='gp_dynamics')\n",
    "\n",
    "k_opt, s_opt = safe_learning.utilities.dlqr(a, b, q, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lyapunov function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov_function = safe_learning.Triangulation(safe_learning.GridWorld(state_limits, 3),\n",
    "                                                vertex_values=[1, 0, 1],\n",
    "                                                name='lyapunov_function')\n",
    "lipschitz_lyapunov = 1.\n",
    "\n",
    "lyapunov = safe_learning.Lyapunov(safety_disc,\n",
    "                                  lyapunov_function,\n",
    "                                  dynamics,\n",
    "                                  lipschitz_dynamics,\n",
    "                                  lipschitz_lyapunov,\n",
    "                                  tau,\n",
    "                                  policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial safe set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov.initial_safe_set = np.abs(lyapunov.discretization.all_points.squeeze()) < 0.05\n",
    "\n",
    "lyapunov.update_safe_set()\n",
    "noisy_dynamics = lambda x, u, noise: true_dynamics(x, u)\n",
    "plotting.plot_lyapunov_1d(lyapunov, noisy_dynamics, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning for the mean dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_dynamics = dynamics.to_mean_function()\n",
    "\n",
    "reward = safe_learning.QuadraticFunction(linalg.block_diag(-q, -r), name='reward_function')\n",
    "\n",
    "value_function = safe_learning.Triangulation(policy_disc,\n",
    "                                             np.zeros(len(policy_disc)),\n",
    "                                             project=True,\n",
    "                                             name='value_function')\n",
    "\n",
    "rl = safe_learning.PolicyIteration(policy, dynamics, reward, value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the dynamics\n",
    "\n",
    "Note that the initial policy is just all zeros!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_STORAGE = {}\n",
    "\n",
    "plotting_discretization = safe_learning.GridWorld(np.vstack((state_limits, action_limits)),\n",
    "                                                 [num_states, num_actions])\n",
    "\n",
    "@safe_learning.utilities.with_scope('get_safe_set')\n",
    "def get_safe_sets(lyapunov, positive=True):\n",
    "    \n",
    "    state_disc = lyapunov.discretization\n",
    "    \n",
    "    safe_states = state_disc.index_to_state(np.where(lyapunov.safe_set))\n",
    "    safe_actions = action_disc.all_points\n",
    "    feed_dict = lyapunov.feed_dict\n",
    "\n",
    "    state_actions = np.column_stack([arr.ravel() for arr in\n",
    "                                     np.meshgrid(safe_states, safe_actions, indexing='ij')])\n",
    "    safe_set = lyapunov.safe_set.reshape(state_disc.num_points)\n",
    "    \n",
    "    storage = safe_learning.utilities.get_storage(_STORAGE, index=lyapunov)\n",
    "    \n",
    "    if storage is None:\n",
    "        tf_state_actions = tf.placeholder(safe_learning.config.dtype,\n",
    "                                          shape=[None, state_actions.shape[1]])\n",
    "    \n",
    "        next_states = lyapunov.dynamics(tf_state_actions)\n",
    "        \n",
    "        mean, bound = next_states\n",
    "        bound = tf.reduce_sum(bound, axis=1)\n",
    "        lv = lyapunov.lipschitz_lyapunov(mean)\n",
    "        values = tf.squeeze(lyapunov.lyapunov_function(mean), 1) + lv * bound\n",
    "        maps_inside = tf.less(values, lyapunov.c_max, name='maps_inside_levelset')\n",
    "    \n",
    "        state, actions = tf.split(tf_state_actions, [1, 1], axis=1)\n",
    "        \n",
    "        dec = lyapunov.v_decrease_bound(state, next_states)\n",
    "        \n",
    "        decreases = tf.less(dec, lyapunov.threshold(state))\n",
    "        \n",
    "        storage = [('tf_state_actions', tf_state_actions),\n",
    "                   ('maps_inside', maps_inside),\n",
    "                   ('mean', mean),\n",
    "                   ('decreases', decreases)]\n",
    "        safe_learning.utilities.set_storage(_STORAGE, storage, index=lyapunov)\n",
    "    else:\n",
    "        tf_state_actions, maps_inside, mean, decreases = storage.values()\n",
    "\n",
    "    # Put placeholder values inside feed_dict and evaluate\n",
    "    feed_dict[tf_state_actions] = state_actions\n",
    "    maps_inside, mean, decreases = session.run([maps_inside, mean, decreases],\n",
    "                                               feed_dict=feed_dict)\n",
    "    \n",
    "    # Add the mean safe set on top\n",
    "    if not positive:\n",
    "        next_state_index = lyapunov.discretization.state_to_index(mean)\n",
    "        safe_in_expectation = lyapunov.safe_set[next_state_index]\n",
    "        maps_inside &= safe_in_expectation\n",
    "        \n",
    "    maps_inside_total = np.zeros(plotting_discretization.nindex, dtype=np.bool)\n",
    "    maps_inside_total = maps_inside_total.reshape(plotting_discretization.num_points)\n",
    "    decreases_total = np.zeros_like(maps_inside_total)\n",
    "    \n",
    "    maps_inside_total[safe_set, :] = maps_inside.reshape(len(safe_states), len(safe_actions))\n",
    "    decreases_total[safe_set, :] = decreases.reshape(len(safe_states), len(safe_actions))\n",
    "\n",
    "    return maps_inside_total, decreases_total\n",
    "\n",
    "\n",
    "@safe_learning.utilities.with_scope('plot_lyapunov_2d')\n",
    "def plot_things():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10), gridspec_kw={'width_ratios': [30, 1]})\n",
    "\n",
    "    # Hide fake cax\n",
    "    cax, cax1 = axes[:, 1]\n",
    "    cax1.set_visible(False)\n",
    "    cax.set_ylabel('Standard deviation $\\sigma$')\n",
    "\n",
    "    ax0, ax1 = axes[:, 0]\n",
    "    ax0.set_ylabel('action')\n",
    "    ax1.set_xlabel('state')\n",
    "    ax1.set_ylabel('$v(\\mathbf{x})$')\n",
    "\n",
    "    ax1.set_ylim(0, np.max(lyapunov.values))\n",
    "    ax1.set_xlim(state_limits.squeeze())\n",
    "    ax0.set_xlim(state_limits.squeeze())\n",
    "    ax0.set_ylim(action_limits.squeeze())\n",
    "    ax0.set_xticks([])\n",
    "\n",
    "    # Hide x-ticks of ax0\n",
    "    plt.setp(ax0.get_xticklabels(), visible=False)\n",
    "\n",
    "    # width between cax and main axis\n",
    "    plt.subplots_adjust(wspace=.05)\n",
    "    feed_dict = lyapunov.feed_dict\n",
    "        \n",
    "    # Plot the dynamics\n",
    "    states = lyapunov.discretization.all_points\n",
    "    state_actions = plotting_discretization.all_points\n",
    "    \n",
    "    storage = safe_learning.utilities.get_storage(_STORAGE, index=lyapunov)\n",
    "    if storage is None:\n",
    "        actions = lyapunov.policy(states)\n",
    "        next_states = lyapunov.dynamics(state_actions)\n",
    "        \n",
    "        storage = [('actions', actions),\n",
    "                   ('next_states', next_states)]\n",
    "        \n",
    "        safe_learning.utilities.set_storage(_STORAGE, storage, index=lyapunov)\n",
    "    else:\n",
    "        actions, next_states = storage.values()\n",
    "    \n",
    "    mean, bound = session.run(next_states, feed_dict=feed_dict)\n",
    "    \n",
    "    # Show the GP variance\n",
    "    img = ax0.imshow(bound.reshape(plotting_discretization.num_points).T,\n",
    "                     origin='lower',\n",
    "                     extent=plotting_discretization.limits.ravel(),\n",
    "                     aspect='auto')\n",
    "    \n",
    "    # Plot the dynamics\n",
    "    ax0.plot(lyapunov.dynamics.X[:, 0],\n",
    "             lyapunov.dynamics.X[:, 1], 'x')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "\n",
    "    safe, safe_expanders = get_safe_sets(lyapunov)    \n",
    "    safe = safe.reshape(plotting_discretization.num_points)\n",
    "    v_dec = safe_expanders.reshape(plotting_discretization.num_points)\n",
    "    \n",
    "    safe_mask = np.ma.masked_where(~safe, safe)\n",
    "    \n",
    "\n",
    "    # Overlay the safety feature\n",
    "    img = ax0.imshow(safe_mask.T,\n",
    "                     origin='lower',\n",
    "                     extent=plotting_discretization.limits.ravel(),\n",
    "                     alpha=0.2,\n",
    "                     cmap=colors.ListedColormap(['white']),\n",
    "                     aspect='auto',\n",
    "                     vmin=0,\n",
    "                     vmax=1)    \n",
    "    \n",
    "    # Overlay the safety feature\n",
    "    if np.any(v_dec):\n",
    "        v_dec_mask = np.ma.masked_where(~v_dec, v_dec)\n",
    "        img = ax0.imshow(v_dec_mask.T,\n",
    "                         origin='lower',\n",
    "                         extent=plotting_discretization.limits.ravel(),\n",
    "                         alpha=0.5,\n",
    "                         cmap=colors.ListedColormap(['red']),\n",
    "                         aspect='auto',\n",
    "                         vmin=0,\n",
    "                         vmax=1)\n",
    "    \n",
    "    is_safe = lyapunov.safe_set\n",
    "    # Plot the Lyapunov function\n",
    "    lyap_safe = np.ma.masked_where(~is_safe, lyapunov.values)\n",
    "    lyap_unsafe = np.ma.masked_where(is_safe, lyapunov.values)\n",
    "\n",
    "    # Plot lines for the boundary of the safety feature\n",
    "    x_min_safe = np.min(states[is_safe])\n",
    "    x_max_safe = np.max(states[is_safe])\n",
    "\n",
    "    ax1.plot(states, lyap_safe, 'r')\n",
    "    ax1.plot(states, lyap_unsafe, 'b')\n",
    "\n",
    "    kw_axv = {'color': 'red',\n",
    "              'alpha': 0.5}\n",
    "    ax0.axvline(x=x_min_safe, ymin=-0.2, ymax=1, clip_on=False, **kw_axv)\n",
    "    ax1.axvline(x=x_min_safe, ymin=0, ymax=1, clip_on=False, **kw_axv)\n",
    "\n",
    "    ax0.axvline(x=x_max_safe, ymin=-0.2, ymax=1, clip_on=False, **kw_axv)\n",
    "    ax1.axvline(x=x_max_safe, ymin=0, ymax=1, clip_on=False, **kw_axv)\n",
    "    \n",
    "    # Plot the current policy\n",
    "    actions = actions.eval(feed_dict=feed_dict)\n",
    "    ax0.step(states, actions, label='safe policy', alpha=0.5)\n",
    "\n",
    "    ax0.legend()\n",
    "    plt.show()\n",
    "\n",
    "# optimize_safe_policy(lyapunov)\n",
    "lyapunov.update_safe_set()\n",
    "plot_things()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning\n",
    "As we sample within this initial safe set, we gain more knowledge about the system. In particular, we iteratively select the state withing the safe set, $\\mathcal{S}_n$, where the dynamics are the most uncertain (highest variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = action_disc.all_points\n",
    "action_variation = safe_learning.GridWorld(np.array(action_limits) / 20, 11).all_points\n",
    "\n",
    "rl_opt_value_function = rl.optimize_value_function()\n",
    "for i in range(3):\n",
    "    rl_opt_value_function.eval(feed_dict=lyapunov.feed_dict)\n",
    "    rl.discrete_policy_optimization(action_space)\n",
    "\n",
    "\n",
    "with tf.variable_scope('add_new_measurement'):\n",
    "        action_dim = rl.policy.output_dim\n",
    "        tf_max_state_action = tf.placeholder(safe_learning.config.dtype,\n",
    "                                             shape=[1, safety_disc.ndim + action_dim])\n",
    "        tf_measurement = true_dynamics(tf_max_state_action)\n",
    "        \n",
    "def update_gp():\n",
    "    \"\"\"Update the GP model based on an actively selected data point.\"\"\"\n",
    "    # Optimize the value/function and policy\n",
    "    rl_opt_value_function.eval(feed_dict=lyapunov.feed_dict)\n",
    "    rl.discrete_policy_optimization(action_space)\n",
    "    \n",
    "    # Get a new sample location\n",
    "    lyapunov.update_safe_set()\n",
    "    max_state_action, _ = safe_learning.get_safe_sample(lyapunov,\n",
    "                                                        action_variation,\n",
    "                                                        action_limits)\n",
    "\n",
    "    # Obtain a measurement of the true dynamics\n",
    "    lyapunov.feed_dict[tf_max_state_action] = max_state_action\n",
    "    measurement = tf_measurement.eval(feed_dict=lyapunov.feed_dict)\n",
    "\n",
    "    # Add the measurement to our GP dynamics\n",
    "    lyapunov.dynamics.add_data_point(max_state_action, measurement)\n",
    "    \n",
    "\n",
    "update_gp()\n",
    "plot_things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    update_gp()\n",
    "    \n",
    "lyapunov.update_safe_set()\n",
    "plot_things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
